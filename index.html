<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="shortcut icon" href="#" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <link
      href="https://fonts.googleapis.com/css?family=Nunito"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />
    <!-- <link rel="icon" type="image/ico" href="/images/favicon.ico" /> -->
    <title>Elf & Myself</title>
  </head>
  <body>
    <nav>
      <img data-aos="fade-up" src="./images/header.png" class="header" /><img
        class="arrow__header"
        src="./images/arrow2.png"
      />
    </nav>

    <div data-aos="fade-up" class="container__videos">
      <div>
        <h1 class="h1__homepage">Demo of project</h1>
        <iframe
          class="container__video"
          src="https://player.vimeo.com/video/783008193?h=bf87c7d5fd"
          frameborder="0"
          allow="autoplay; fullscreen; picture-in-picture"
          allowfullscreen
          title="Making of Party Popper"
        ></iframe>
      </div>

      <div>
        <h1 class="h1__homepage">Making of project</h1>
        <iframe
          class="container__video"
          src="https://player.vimeo.com/video/782550067?h=8cebc1dbc8"
          frameborder="0"
          allow="autoplay; fullscreen; picture-in-picture"
          allowfullscreen
          title="Making of Elf"
        ></iframe>
      </div>
    </div>
    <div data-aos="fade-up" class="container__text">
      <h1 class="h1__homepage">Goal and motivation of project</h1>
      <div class="container__text--white">
        Elf & Myself is a Christmas experience that bridges the gap between the
        virtual- and outside world. A VR-user steps into a cozy christmas cabin
        where they can decorate a tree, while an outside user takes the role of
        an elf that peeks into the windows of the cabin and controls their point
        of view through the gyroscope in a smartphone.
        <br /><br />
        One of the motivations behind Elf & Myself was that we wanted to close
        the distance by breaking the wall between the VR-player and the
        audience. VR is usually quite an isolated experience where interactions
        with the outside world and observers is non-existent or restricted.
        Instead of merely casting the HMD-users view to a public screen, we
        wanted to provide the outside observer with a higher sense of immersion
        by allowing them “peep” into the VR-world and control their point of
        view through embodied interaction. The outside spectator’s position was
        also visualized in the VR-space.
        <br /><br />
        Our goal was therefore to create a closer connection between the
        audience and the player immersed in VR. Whilst doing this we wanted to
        deepen our knowledge in graphics and interactions by exploring new
        techniques and novel means of interaction. We also wanted to work with
        lights and shadows to create a more realistic, immersive experience and
        to create a cozy atmosphere in line with our Christmas theme. It was
        created as a part of the course Advanced Graphics and Interaction in
        just 4 weeks during the winter of 2022.
      </div>
    </div>
    <div data-aos="fade-up" class="container__images">
      <img class="container__images__image" src="./images/decor.png" />
      <img class="container__images__image" src="./images/outside.png" />
      <img class="container__images__image" src="./images/looking.png" />
    </div>
    <div data-aos="fade-up" class="container__text">
      <h1 class="h1__homepage">
        Graphics and interaction technologies used and developed
      </h1>
      <div data-aos="fade-in" class="container__text--white">
        The software Ready Player Me offers customizable 3D avatars created for
        VR experiences (especially the Metaverse). The avatar was imported using
        the Ready Player Me SDK and acts like a normal 3D asset in Unity. For
        the skeletal movement of the avatar, the Animation Rigging package was
        used. Animation rigging was used to define the bone structure of the
        character and assigning skin weights to the mesh, which determines how
        the vertices of the mesh are deformed when the bones move. Once the
        character was rigged, it could be animated by moving the bones in the
        hierarchy, which will cause the mesh to deform and create movements. For
        us, the movements of the avatar were based on the hands and head
        movements of the player connected with the Oculus VR kit. Using the
        avatar from Ready Player Me allowed us to spend less time on advanced
        skeleton modeling in Blender and more time on other graphics and
        interactions that were crucial for the experience.
        <br /><br />
        To synchronize Elf & Myself between the VR-player and the audience, we
        first wanted to videostream an in-game camera from Unity that the
        audience could see. The problem was that we had to stream from the
        built-in app in the Oculus Quest 2 headset. This showed to be tricky, as
        there were no current solutions to stream an in-game camera from an
        Android build, only from PC. Instead, we used NormCore to keep the state
        of all clients the same. Normcore is a networking plugin and hosting
        service created by Normal and is built to enable multiplayer gameplay in
        a quick and efficient way. Using NormCore, we could use different
        scripts and solutions to send data about different objects' orientation
        in space to keep everything up to date, showing the same gameplay on
        both the audience and VR-player view.
        <br /><br />
        All models were created using Blender, additionally some procedural
        textures were developed, such as the gingerbread-cookie and the
        fireplace stone. In order to export and import the procedural textures
        from Blender to Unity, the objects were UV unwrapped and the textures
        baked into images, a diffuse map for color information and a normal map.
        These were then applied to the imported model in Unity. This not only
        enabled us to use the procedural textures created in Blender, but can
        also save render time.
        <br /><br />
        We did use some external resources. The crackling noise of the fireplace
        is from freesound.org. The avatar is from Ready Player Me, which is a
        Metaverse avatar creator.
      </div>
    </div>
    <div data-aos="fade-up" class="container__images">
      <img class="container__images__tester" src="./images/tester2.png" />
      <img class="container__images__tester" src="./images/tester1.png" />
      <img class="container__images__tester" src="./images/tester3.png" />
    </div>
    <div data-aos="fade-up" class="container__text">
      <h1 class="h1__homepage">Challenges & Obstacles</h1>
      <div class="container__text--white">
        In order to break down the wall between the HMD user and the outside
        spectator we deemed it critical to create realistic avatars representing
        both users. We decided to use the Oculus Quest 2 since we are familiar
        with it and due to the mobility afforded by it being able to run it
        cordless. However, that also meant we only had three points of tracking
        which was a big challenge when creating the representation of the
        VR-player. It is evident that it is very challenging, if not impossible,
        to create fully realistic human movement based on only three points of
        tracking. With regards to the outside spectator we only had one
        dimension of tracking - the rotation, so it was also a big challenge to
        represent that user in VR.
        <br /><br />
        Due to the time restriction of this project, it was also a challenge to
        prioritize the work and still explore advanced and, to us, novel
        graphics and interaction techniques. Based on previous experience, we
        knew that when working with the Oculus Quest 2, another big challenge is
        to balance graphics and performance.
        <br /><br />
        The biggest obstacle was the restricted time we had to develop this
        project, 4 weeks. As mentioned, we had to prioritize our work. The end
        product has managed to achieve what we set out to do, that is to narrow
        the bridge between the virtual- and outside-world using embodied
        interaction. However, it would have benefited from more throughout user
        testing since it does contain some bugs and is somewhat unreliable.
        Another obstacle was that all of us advanced our Blender skills out of
        interest and therefore wanted to add more things into the scene which
        was troubling for the headsets performance. When putting everything
        together, things had to be recreated into more low poly versions in
        order to work together.
      </div>
    </div>
    <div data-aos="fade-up" class="container__text">
      <h1 class="h1__homepage">Lessons learned</h1>
      <div class="container__text--white">
        For this project, we really had to adapt to the limitations of the
        Oculus Quest 2 graphic constraints, and learned how we could lower the
        resolution of our 3d models to work around this. For example, the
        surrounding trees initially had snow on them but each tree was then made
        up of 33 thousand vertices. When replacing the snow by just adding white
        cones and adding a decimate modifier reduced the number of vertices to
        ≈250. Also, we used baked lightning and used low resolution shadows in
        order for the game to run smoothly. This taught us of the importance of
        always testing our stuff in the final environment it is intended for,
        which in this case was VR, so you don’t fumble in the dark with how the
        meshes and textures you’ve created might impact the performance.
        Additionally, we learnt the importance of being careful with scripts.
        One script that we had caused a major performance issue as an if
        statement was checked indefinitely. When that was fixed, the performance
        increased tremendously, so we will be more careful about how we code our
        scripts in the future.

        <br /><br />
        We also learned how to bake textures in Blender. Although it is quite a
        time consuming and unintuitive process, it enabled us to use the
        procedural textures created in Blender. This also didn’t affect the
        performance of the headset as much as other graphics.
        <br /><br />
        As it was everyone’s first time making a multiplayer game, we learnt how
        tricky it could be to synchronize all the objects between the clients.
        If two clients were interacting with an object at the same time, they
        interfered with each other and a lot of bugs started to show. We managed
        to solve the synchronization issues by having the clients claim
        ownership of an object when interacted with, and that way two clients
        couldn’t interact with an object at the same time.
        <br /><br />
        To allow the outside observer to control their point of view on a fixed
        screen through embodied interaction, we used the gyroscope sensor in a
        smartphone. We tried making this compatible for both iPhone and Android,
        but struggled with Apple’s security measures and decided to only
        implement it for Android. Using the Unity Remote 5 app we learned how to
        connect the phone to the Unity Editor on the computer and access the
        gyroscope data. We then calibrated the data to make sense in relation to
        the phone's rotation and the rotation of the camera in the VR scene. The
        resulting rotational values were not only applied to control the camera
        in the scene but also used to transform the rotation of the Elf’s head.
        <br /><br />
        Finally, we realized how difficult it can be to rig an avatar in VR. As
        you only have the headset and the two controllers, it makes it hard and
        essentially impossible to mimic the movement of arms and legs in VR. We
        understood why many games have chosen to hide the legs of avatars, and
        to not let the player themselves see their own arms. It will be
        interesting to see how this problem will be solved in the future.
      </div>
    </div>
    <div data-aos="fade-up" class="container__related-work">
      <h1 class="h1__homepage">Related work</h1>
      <div class="container__related">
        <div class="container__text--green">
          <h3>
            TeleSight: enabling asymmetric collaboration in VR between HMD user
            and Non-HMD users
          </h3>
          <h3 class="container__h3">
            Taichi Furukawa, Daisuke Yamamoto, Moe Sugawa, Roshan Peiris, Kouta
            Minamizawa. 2019.
          </h3>
          <div>
            This paper proposed a concept for breaking the wall between
            HMD-users and non-HMD users by introducing a physical interaction
            layer in the form of an avatar robot that mimics the motions of the
            HMD user. The non-HMD user can interact with the HMD-user by
            interaction with the robot, e.g. cover the eyes of the robot and
            obscure the view of the HMD-user. Additionally, there is a visual
            layer in the form of a projection of the VR-space to provide outside
            spectators with a view of the VR-environment. This was relevant for
            our project since our aim was also to immerse the non-HMD user in
            the VR-space through embodied interaction. In our case, the embodied
            interaction through physical movement will control the non-HMD
            user's field of view.
          </div>
        </div>
        <div class="container__text--green">
          <h3>
            Am I a Bunny?: The Impact of High and Low Immersion Platforms and
            Viewers' Perceptions of Role on Presence, Narrative Engagement, and
            Empathy during an Animated 360° Video
          </h3>
          <h3 class="container__h3">
            Samantha W. Bindman, Lisa M. Castaneda, Mike Scanlon, and Anna
            Cechony. 2018.
          </h3>
          <div>
            This paper explored the sense of presence and understanding of ones
            narrative role in a virtual environment in relation to the level of
            immersion of the platform used. Participants reactions where
            compared between a low-level immersion platform (viewing a 360 video
            on a smartphone) and a high-level immersion platform (a HMD). The
            findings of the study suggest that while a high-level immersion
            platform can easily increase the user’s sense of presence in a
            virtual environment, it is not significantly related to user’s
            narrative engagement and empathy towards the environment and story.
            Here, the more important aspect was to convey the user’s role in
            terms of being just an observer or if they experienced themselves to
            be an active character in the scene
            <br /><br />
            This can be connected to out project, as we wanted to establish
            immersion for both a non-HMD user and a HMD-user. The study suggests
            that it is possible to immerse a non-HMD user in a virtual
            environment as long as their role is conveyed clearly to them. This
            is what we aimed to accomplish.
          </div>
        </div>
        <div class="container__text--green">
          <h3>
            Spectator View: Enabling Asymmetric Interaction between HMD Wearers
            and Spectators with a Large Display
          </h3>
          <h3 class="container__h3">
            Finn Welsford-Ackroyd, Andrew Chalmers, Rafael Kuffner dos Anjos,
            Daniel Medeiros, Hyejin Kim, and Taehyun Rhee. 2021.
          </h3>
          <div>
            This study connected a HMD-user with a non-HMD user through a large
            display, aiding collaboration between the person in the virtual
            environment and the spectator. The spectator had a camera view
            separate from that of the HMD-user, which they were free to control
            the orientation of. Some elements to support collaboration where
            also included in the setup, like an ability for the spectator to
            point to different parts of the VR scene which the HMD-user was made
            aware of.
            <br /><br />Although our project does not focus on collaboration per
            se, the connection between a spectator and a HMD-user is still
            something we can draw inspiration from. We thus wanted to give the
            person outside of VR some level of control of their camera view, as
            well as separating it from the view of the HMD-user. We also wanted
            to implement a spectator screen like the one in this study, as
            opposed to for example only viewing the scene from a smartphone,
            since the screen approach was more related to the window metaphor
            that we wanted to convey.
          </div>
        </div>
      </div>
    </div>
    <button class="scroll__btn" id="top-button">
      <img class="scroll__btn__image" src="./images/arrow2.png" />
    </button>
    <h1 data-aos="fade-up" class="about__h1">The Team</h1>
    <section data-aos="fade-up" class="about">
      <div class="about__person">
        <img src="./images/agnes.png" class="about__person__image" />
        <h2>Agnes Shutrick</h2>
        <h3>agnesben@kth.se</h3>
        <div class="about__person__text">
          VR avatar and tracking; candy cane ornament, table, carpet and
          christmas tree models; VR player interactions
        </div>
      </div>
      <div class="about__person">
        <img src="./images/maria.png" class="about__person__image" />
        <h2>Maria Jacobson</h2>
        <h3>majacobs@kth.se</h3>
        <div class="about__person__text">Elf model; bell ornament</div>
      </div>
      <div class="about__person">
        <img src="./images/ebba.png" class="about__person__image" />
        <h2>Ebba Rovig</h2>
        <h3>rovig@kth.se</h3>
        <div class="about__person__text">
          Cabin and outside environment, fireplace, stockings models, initial
          connection and reading of gyroscope data from phone to Unity
        </div>
      </div>
      <div class="about__person">
        <img src="./images/erik.png" class="about__person__image" />
        <h2>Erik Meurk</h2>
        <h3>emeurk@kth.se</h3>
        <div class="about__person__text">
          Implemented multiplayer; VR player interactions; VR avatar tracking;
        </div>
      </div>
      <div class="about__person">
        <img src="./images/amanda.png" class="about__person__image" />
        <h2>Amanda Lindqvist</h2>
        <h3>amlindqv@kth.se</h3>
        <div class="about__person__text">
          Implementation and calibration of rotation of Elf i.e., the non-HMD
          user; gingerbread-ornament, candelabra, and chair models.
        </div>
      </div>
    </section>
    <footer>&copy; Copyright 2022 Elf&Myself</footer>
    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
    <script>
      AOS.init();
    </script>
    <script>
      document
        .getElementById("top-button")
        .addEventListener("click", function () {
          window.scrollTo({
            top: 0,
            left: 0,
            behavior: "smooth",
          });
        });

      window.addEventListener("scroll", function () {
        var scrollPercentage =
          (document.documentElement.scrollTop + document.body.scrollTop) /
          (document.documentElement.scrollHeight -
            document.documentElement.clientHeight);
        if (scrollPercentage > 0.1) {
          document.querySelector(".scroll__btn").style.display = "block";
        } else {
          document.querySelector(".scroll__btn").style.display = "none";
        }
      });
    </script>
  </body>
</html>
